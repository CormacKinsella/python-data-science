{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for machine learning\n",
    "\n",
    "## Fundamentals of machine learning\n",
    "\n",
    "- ML is everywhere in the modern world\n",
    "- Machines can do repetitive tasks quickly\n",
    "- We traditionally give machines input + instructions -> output\n",
    "- 1959: Arthur Samuel - can machines infer logic instead of being given instructions?\n",
    "- Could we give them just input data, and output from previously finished tasks, and let them figure out the best instructions to give the same output based on the data provided? E.g., is a linear model most appropriate?\n",
    "- Once a model is trained, we could just give it input data\n",
    "- This is supervised learning -> applications include spam filtering, image analysis, and text prediction\n",
    "- Unsupervised learning -> we just give it input data, and ask it to find patterns -> e.g., movie recommendations on Netflix\n",
    "- Reinforcement learning -> positive or negative feedback, to learn the optimal path to take given the environment\n",
    "\n",
    "### Terminology\n",
    "\n",
    "- AI is a catch-all term -> all ML is AI, not all AI is ML\n",
    "- ML encompasses unsupervised, supervised, and reinforcement learning paradigms (the three core categories of ML)\n",
    "- A further term within machine learning is deep learning -> all deep learning is ML, not all DL is ML\n",
    "\n",
    "#### Unsupervised learning\n",
    "- the process of building descriptive models\n",
    "- identify patterns in unlabelled data\n",
    "- used to summarise and group data in new ways\n",
    "- useful to uncover patterns that might be informative for business purposes\n",
    "- e.g., these people have x behaviour, these have y -> offer x this, and y this\n",
    "\n",
    "#### Supervised learning \n",
    "- training a predictive model\n",
    "- learn patterns from previously labelled data, and then:\n",
    "- assign label to unlabelled data, based on the historical data\n",
    "- input = independent variable, output = dependent variable. Together, these make training data.\n",
    "- Example: previous performance of some gamblers, with preferences, cash amount (independent variables) + profit or loss for the business (dependent variables). Train the model on this.\n",
    "- Assess performance of the model -> give it only input data, hiding the outcome. What does it predict? \n",
    "- This gives the predictive accuracy, e.g. 99%. \n",
    "- A model is \"learning\" if it's performance at a task improves with experience\n",
    "- From this we can take E - experience, T - class of task, & P - performance measure (predictive accuracy)\n",
    "\n",
    "#### Reinforcement learning\n",
    "- learning to make decisions on the basis of interactions\n",
    "- Objective 1: find unknown solutions to existing problems (e.g. a chess computer)\n",
    "- Objective 2: Find solutions to unpredicted problems\n",
    "- Two entities in RL. Agent & environment.\n",
    "- Agent interacts with env by acting (its objective is to maximise rewards to itself)\n",
    "- Environment provides feedback to agent (state - describes impact of previous action, and possible next actions + reward - the numeric reward)\n",
    "- Exploitation = choosing the current action that maximises the reward\n",
    "- Exploration = choosing other actions that do not necessarily appear to maximise the reward (choosing action without considering reward)\n",
    "- The exploitation vs exploration reward problem. The simplest agent model will always choose to maximise score, rather than explore other lower scoring avenues that may be longer term higher scoring. A balanced approach is likely more successful. \n",
    "\n",
    "\n",
    "#### Deep learning\n",
    "- deep because the algorithm network has many layers\n",
    "- broad term in itself, a form of ML based on the human brain/animal nervous system\n",
    "- feature learning or representation learning, which can be supervised, semi-supervised, or unsupervised\n",
    "- progressive extraction of higher-level features from raw input\n",
    "- can use \"artificial neural networks\", or \"neural nets\", algorithms with inter-connected nodes\n",
    "- edges have associated weights, and the network defines rules for data to be passed from an input layer to the output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to the ML process\n",
    "\n",
    "### 1. Data collection\n",
    "\n",
    "- unlabelled data (unsupervised)\n",
    "- labelled historical data (supervised)\n",
    "- data that helps the agent learn which actions yield most reward (reinforcement)\n",
    "\n",
    "Considerations: data accuracy (i.e., is it really \"ground truth data\"), relevance (is it important data for the model aim?), quantity (some models need little, some need a lot), variability (do we capture the full data range?), ethics (informed consent, biases in collection leading to biases in policy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNormally instead of making DFs from dictionaries\\nwe\\'d use the pandas import functionality (many file types supported)\\nE.g.\\nbrics2 = pd.read_csv(\"data.csv\")\\nbrics2 = pd.read_excel(\"data.xlsx, sheet_name = \"brics_data\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing data in pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Generate pandas series from list\n",
    "members_list = [\"Brazil\",\"Russia\",\"India\",\"China\",\"South Africa\"]\n",
    "brics1 = pd.Series(members_list) # Capital import\n",
    "print(type(brics1))\n",
    "\n",
    "# Pandas data frame, 2D-data structure. Essentially many series with one index.  \n",
    "members_dictionary = {\"country\": [\"Brazil\", \"Russia\", \"India\", \"China\", \"South Africa\"],\n",
    "        \"capital\": [\"Brasilia\", \"Moscow\", \"New Delhi\", \"Beijing\", \"Pretoria\"],\n",
    "        \"gdp\": [2750, 1658, 3202, 15270, 370],\n",
    "        \"literacy\":[.944, .997, .721, .964, .943],\n",
    "        \"expectancy\": [76.8, 72.7, 68.8, 76.4, 63.6],\n",
    "        \"population\": [210.87, 143.96, 1367.09, 1415.05, 57.4]}\n",
    "\n",
    "brics2 = pd.DataFrame(members_dictionary)\n",
    "print(type(brics2))\n",
    "\n",
    "\"\"\"\n",
    "Normally instead of making DFs from dictionaries\n",
    "we'd use the pandas import functionality (many file types supported)\n",
    "E.g.\n",
    "brics2 = pd.read_csv(\"data.csv\")\n",
    "brics2 = pd.read_excel(\"data.xlsx, sheet_name = \"brics_data\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data exploration\n",
    "\n",
    "- understand data: describe and visualise it\n",
    "- what kind of data is it?\n",
    "- duplicates, missing data, inconsistent data?\n",
    "\n",
    "In machine learning:\n",
    "\n",
    "- an **instance** is a row of data (also called a record or observation). Each is an independent example of the target concept represented by the dataset, e.g. a customer and their data\n",
    "- each instance is described by **features** (also known as attributes or variables). These are the properties of an instance. E.g. name, height, weight, income. Can be continuous, categorical, etc. \n",
    "- the **response** (if continuous) or **class** (if categorical) is a feature that holds the *dependent variable* described by the *independent variables*\n",
    "- the **dimensionality** of datasets is related to the number of features. More features = more data about each instance, more dimensionality, and more computational complexity.\n",
    "- the **sparcity** and **density** describe the data completeness of a dataset. 30% missing or incomplete data = 30% sparse, or 70% dense dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   country     5 non-null      object \n",
      " 1   capital     5 non-null      object \n",
      " 2   gdp         5 non-null      int64  \n",
      " 3   literacy    5 non-null      float64\n",
      " 4   expectancy  5 non-null      float64\n",
      " 5   population  5 non-null      float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BrandName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amana</th>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asko</th>\n",
       "      <td>2.525000</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beko</th>\n",
       "      <td>2.133333</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blomberg</th>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bosch</th>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Volume            \n",
       "               mean median  max\n",
       "BrandName                      \n",
       "Amana      4.250000   4.25  4.3\n",
       "Asko       2.525000   2.70  2.7\n",
       "Beko       2.133333   2.00  2.5\n",
       "Blomberg   2.300000   2.50  2.5\n",
       "Bosch      2.200000   2.20  2.2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarising and visualising data with Pandas\n",
    "\n",
    "\"\"\"\n",
    "Numerous methods are available for pandas dataframe exploration:\n",
    "https://pandas.pydata.org/docs/reference/frame.html\n",
    "\"\"\"\n",
    "\n",
    "# Pandas info method\n",
    "brics2.info()\n",
    "\n",
    "# Head method to peek\n",
    "brics2.head()\n",
    "\n",
    "# Descibe method to get data aggregations, for whole dfs or columns thereof\n",
    "brics2.describe()\n",
    "brics2[['literacy']].describe() # can also do brics2.literacy.describe(), comparable output?\n",
    "\n",
    "# Further column exploration methods\n",
    "brics2[['country']].value_counts()  # For a column, count occurences of each value\n",
    "brics2.sort_values(by=['literacy'], ascending=False)\n",
    "brics2[['expectancy']].mean()\n",
    "\n",
    "# For more complex dataframes, we can normalise\n",
    "product = pd.read_csv(\"data/washers.csv\")\n",
    "product[['BrandName']].value_counts()\n",
    "product[['BrandName']].value_counts(normalize = True)\n",
    "\n",
    "# Groupby - to determine group level summaries/aggregations\n",
    "product.groupby('BrandName')[['Volume']].mean()\n",
    "product.groupby('BrandName')[['Volume']].mean().sort_values(by = 'Volume')\n",
    "\n",
    "# Multiple aggregations with the .agg method\n",
    "product.groupby('BrandName')[['Volume']].agg(['mean','median','max']).head()\n",
    "\n",
    "\"\"\"\n",
    "Visualisation of data:\n",
    "\n",
    "Comparison of groups - boxplot\n",
    "Relationships - scatter/line\n",
    "Distributions - histograms\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preparation (up to here, = 80% of the time!)\n",
    "\n",
    "- Resolve problems (missing data, noisy data, outlier, class imbalance)\n",
    "- structuring it to be easy to use (normalise, reduction etc.)\n",
    "\n",
    "### 4. Modeling\n",
    "\n",
    "- apply a machine learning approach to the data\n",
    "\n",
    "### 5. Evaluation\n",
    "\n",
    "- assess how well it worked, e.g. supervised, are labels or values correct for unseen data?\n",
    "- unsupervised: a good model is one that makes sense\n",
    "- iterate back to the modeling step\n",
    "\n",
    "### 6. Actionable insight\n",
    "\n",
    "- identify a course of action based on the model\n",
    "- do we deploy the model?\n",
    "- what do we do with the insights from the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks\n",
    "\n",
    "\n",
    "\n",
    "# Generative artificial neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SciKit-Learn\n",
    "- Pytorch\n",
    "- TensorFlow\n",
    "- matplotlib\n",
    "- Pandas/NumPy/SciPy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
